<?xml version="1.0" encoding="utf-8"?>
<!-- Created by Leo: http://leoeditor.com/leo_toc.html -->
<leo_file xmlns:leo="http://leoeditor.com/namespaces/leo-python-editor/1.1" >
<leo_header file_format="2" tnodes="0" max_tnode_index="0" clone_windows="0"/>
<globals body_outline_ratio="0.5" body_secondary_ratio="0.5">
	<global_window_position top="50" left="50" height="500" width="700"/>
	<global_log_window_position top="0" left="0" height="0" width="0"/>
</globals>
<preferences/>
<find_panel_settings/>
<vnodes>
<v t="karstenw.20230316191437.2" a="E"><vh>perception</vh>
<v t="karstenw.20230316191501.1" a="E"><vh>@clean __init__.py</vh>
<v t="karstenw.20230316191528.1"><vh>Declarations</vh></v>
<v t="karstenw.20230417110004.1"><vh>+ GLOBALS +</vh></v>
<v t="karstenw.20230318131229.1"><vh>+ TOOLS +</vh>
<v t="karstenw.20230316191631.1" a="E"><vh>py3</vh>
<v t="karstenw.20230316191639.1"><vh>py3 stuff</vh></v>
<v t="karstenw.20230316191528.2"><vh>cmp_to_key</vh></v>
<v t="karstenw.20230316191528.3"><vh>sortlist</vh></v>
</v>
<v t="karstenw.20230316191528.4"><vh>nodecompare</vh></v>
<v t="karstenw.20230316191528.6"><vh>makeunicode</vh></v>
<v t="karstenw.20230316191528.7"><vh>hashFromString</vh></v>
<v t="karstenw.20230316191528.8"><vh>range_</vh></v>
</v>
<v t="karstenw.20230316191737.1"><vh>+ BASIC PROPERTIES +</vh></v>
<v t="karstenw.20230316191816.1"><vh>+ Cache +</vh>
<v t="karstenw.20230403165828.1"><vh>_newpath  ## UNUSED ##</vh></v>
<v t="karstenw.20230316191528.9"><vh>_path</vh></v>
<v t="karstenw.20230316191528.10"><vh>cache</vh></v>
<v t="karstenw.20230316191528.11"><vh>cached</vh></v>
<v t="karstenw.20230316191528.12"><vh>in_cache</vh></v>
<v t="karstenw.20230316191528.13"><vh>clear_cache</vh></v>
<v t="karstenw.20230505121633.1"><vh>cachefiles</vh></v>
</v>
<v t="karstenw.20230318130644.1"><vh>+ API +</vh>
<v t="karstenw.20230505102222.1" a="E"><vh>+ OFF +</vh>
<v t="karstenw.20230316191528.14"><vh>is_robot - OFF</vh></v>
<v t="karstenw.20230330102024.1" a="E"><vh>+ QUERY CONCEPTNET API +</vh>
<v t="karstenw.20230330142517.1"><vh>download_url</vh></v>
<v t="karstenw.20230330170426.1"><vh>make_url</vh></v>
<v t="karstenw.20230330103714.1"><vh>get_query_cnconcept</vh></v>
<v t="karstenw.20230329145218.1"><vh>get_query_lang_concept</vh></v>
<v t="karstenw.20230329150729.1"><vh>get_egde_nodes</vh></v>
<v t="karstenw.20230329150748.1"><vh>get_next_page</vh></v>
<v t="karstenw.20230330104408.1"><vh>get_all_pages</vh></v>
<v t="karstenw.20230329135420.1"><vh>query_cn   --   api.conceptnet.io</vh></v>
<v t="karstenw.20230403170112.1" a="E"><vh>+ NEW STUFF UNSORTED +</vh>
<v t="karstenw.20230403170027.1"><vh>getoldwords</vh></v>
<v t="karstenw.20230403170129.1"><vh>getpatternwords</vh></v>
<v t="karstenw.20230403170137.1"><vh>scancache</vh></v>
<v t="karstenw.20230403170145.1"><vh>loadcache</vh></v>
<v t="karstenw.20230403170157.1"><vh>ask</vh></v>
</v>
</v>
</v>
<v t="karstenw.20230316191528.15"><vh>normalize</vh></v>
<v t="karstenw.20230316191528.16"><vh>class InternetError</vh></v>
<v t="karstenw.20230316191528.17"><vh>class Rule</vh>
<v t="karstenw.20230316191528.18"><vh>__init__</vh></v>
<v t="karstenw.20230316191528.19"><vh>__repr__</vh></v>
</v>
<v t="karstenw.20230316191528.20" a="E"><vh>class Rules</vh>
<v t="karstenw.20230316191528.21"><vh>_concepts</vh></v>
<v t="karstenw.20230316191528.22"><vh>disambiguate</vh></v>
</v>
</v>
<v t="karstenw.20230417110310.1" a="E"><vh>+ QUERY SQLITE +</vh>
<v t="karstenw.20230417105918.1"><vh>query_cnr</vh></v>
</v>
<v t="karstenw.20230316194255.1" a="E"><vh>+ CONCEPT CLUSTER +</vh>
<v t="karstenw.20230316191528.24"><vh>has_rule</vh></v>
<v t="karstenw.20230316191528.25"><vh>is_a</vh></v>
</v>
<v t="karstenw.20230316194456.1"><vh>+ NODE RULE RETRIEVAL +</vh>
<v t="karstenw.20230316191528.39"><vh>enumerate_rules</vh></v>
<v t="karstenw.20230316191528.40"><vh>specific parts opposites properties associations aliases effects type whole objects causes</vh></v>
</v>
<v t="karstenw.20230316194540.1" a="E"><vh>+ CLUSTER +</vh>
<v t="karstenw.20230316191528.51"><vh>style</vh></v>
<v t="karstenw.20230316191528.52"><vh>add_rule</vh></v>
<v t="karstenw.20230316191528.53"><vh>cluster</vh></v>
</v>
<v t="karstenw.20230316194605.1" a="E"><vh>+ CLUSTER HEURISTICS +</vh>
<v t="karstenw.20230316191528.54"><vh>proper_nouns</vh></v>
<v t="karstenw.20230316191528.55"><vh>proper_leaves</vh></v>
<v t="karstenw.20230316191528.56"><vh>neighbors</vh></v>
<v t="karstenw.20230316191528.57"><vh>class cost</vh>
<v t="karstenw.20230316191528.58"><vh>__init__</vh></v>
<v t="karstenw.20230316191528.59"><vh>tax</vh></v>
<v t="karstenw.20230316191528.60"><vh>__call__</vh></v>
</v>
<v t="karstenw.20230316191528.61"><vh>graph_enumerate_rules</vh></v>
<v t="karstenw.20230316191528.62"><vh>graph_properties</vh></v>
<v t="karstenw.20230316191528.63"><vh>graph_specific</vh></v>
<v t="karstenw.20230316191528.64"><vh>graph_objects</vh></v>
</v>
<v t="karstenw.20230316194702.1" a="E"><vh>+ TAXONOMY +</vh>
<v t="karstenw.20230316191528.65"><vh>taxonomy</vh></v>
<v t="karstenw.20230316191528.66"><vh>class _range</vh>
<v t="karstenw.20230316191528.67"><vh>__init__</vh></v>
<v t="karstenw.20230316191528.68"><vh>append</vh></v>
<v t="karstenw.20230316191528.69"><vh>_hyponyms</vh></v>
<v t="karstenw.20230316191528.70"><vh>__getattr__</vh></v>
<v t="karstenw.20230316191528.71"><vh>__call__</vh></v>
</v>
</v>
<v t="karstenw.20230316194807.1"><vh>+ INDEX +</vh>
<v t="karstenw.20230316191528.72"><vh>class _index</vh>
<v t="karstenw.20230316191528.73"><vh>__init__</vh></v>
<v t="karstenw.20230316191528.74"><vh>_get_name</vh></v>
<v t="karstenw.20230316191528.75"><vh>_set_name</vh></v>
<v t="karstenw.20230316191528.76"><vh>build</vh></v>
<v t="karstenw.20230316191528.77"><vh>shortest_path</vh></v>
<v t="karstenw.20230316191528.78"><vh>nearest</vh></v>
<v t="karstenw.20230316191528.79"><vh>sort_by_distance</vh></v>
</v>
<v t="karstenw.20230316191528.80"><vh>_build_properties_index</vh></v>
</v>
<v t="karstenw.20230316194837.1" a="E"><vh>+ SOLVER +</vh>
<v t="karstenw.20230316191528.81"><vh>add_method</vh></v>
<v t="karstenw.20230316191528.82"><vh>score</vh></v>
<v t="karstenw.20230316191528.83"><vh>class IndexError</vh></v>
<v t="karstenw.20230316191528.84"><vh>class MethodError</vh></v>
<v t="karstenw.20230316191528.85"><vh>class analysis</vh></v>
<v t="karstenw.20230316191528.86" a="E"><vh>class _solver</vh>
<v t="karstenw.20230316191528.87"><vh>__init__</vh></v>
<v t="karstenw.20230316191528.88"><vh>_prepare</vh></v>
<v t="karstenw.20230316191528.89"><vh>_retrieve</vh></v>
<v t="karstenw.20230316191528.90"><vh>sort_by_relevance</vh></v>
</v>
</v>
<v t="karstenw.20230316194920.1" a="E"><vh>+ ANALOGY +</vh>
<v t="karstenw.20230316191528.91" a="E"><vh>class _analogy</vh>
<v t="karstenw.20230316191528.92"><vh>__init__</vh></v>
<v t="karstenw.20230316191528.93"><vh>_best_first</vh></v>
<v t="karstenw.20230316191528.94"><vh>__call__</vh></v>
</v>
<v t="karstenw.20230330100356.1" a="E"><vh>+ SEARCH-MATCH-PARSE +</vh>
<v t="karstenw.20230316191528.95"><vh>search_match_parse</vh></v>
</v>
<v t="karstenw.20230316191528.96"><vh>count</vh></v>
<v t="karstenw.20230316191528.97"><vh>clean</vh></v>
</v>
<v t="karstenw.20230316195011.1" a="E"><vh>+ SIMILE +</vh>
<v t="karstenw.20230316191528.98"><vh>suggest_properties</vh></v>
<v t="karstenw.20230316191528.99"><vh>suggest_objects</vh></v>
</v>
<v t="karstenw.20230316195027.1" a="E"><vh>+ COMPARATIVE +</vh>
<v t="karstenw.20230316191528.100" a="E"><vh>class compare_concepts</vh>
<v t="karstenw.20230316191528.101"><vh>__init__</vh></v>
<v t="karstenw.20230316191528.102"><vh>graph</vh></v>
<v t="karstenw.20230316191528.103"><vh>rank</vh></v>
</v>
<v t="karstenw.20230316191528.104"><vh>suggest_comparisons</vh></v>
</v>
<v t="karstenw.20230316195056.1"><vh>+ OUTRO +</vh></v>
</v>
</v>
</vnodes>
<tnodes>
<t tx="karstenw.20230316191437.2"></t>
<t tx="karstenw.20230316191501.1">@language python
@tabwidth -4
@others
</t>
<t tx="karstenw.20230316191528.1"># coding: utf-8

### PERCEPTION ########################################################################################
# Analysis tools for working with data from http://nodebox.net/perception in NodeBox.
# The library is roughly organised in 5 parts that add up to the final solver object:
# 1) query()   : returns lists of rules form the online database, using a caching mechanism.
# 2) cluster() : returns a graph objects based on a cluster of rules around a central concept.
# 3) range()   : find sibling concepts (e.g. fonts, movies, colors, trees) using a taxonomy graph.
# 4) index     : object for building and searching cached indices of shortest paths.
# 5) cost      : object that simplifies the creation of path search heuristics.
# =&gt; solver    : object for inferring knowledge from the database, using clusters,
#                indices and ranges.

### CREDITS ########################################################################################

# Copyright (c) 2008 Tom De Smedt.
# See LICENSE.txt for details.

__author__    = "Tom De Smedt"
__version__   = "beta+"
__copyright__ = "Copyright (c) 2008 Tom De Smedt"
__license__   = "GPL"

########################################################################################

import os
import time

startimport = time.time()


import linguistics
import conceptnetreader

# import md5
import hashlib
import glob
from re import sub
from socket import setdefaulttimeout
# from urllib2 import urlopen
from urllib.request import urlopen
import urllib.parse

import pickle
import json
import pprint
pp = pprint.pprint

from random import random
import requests
import pdb


import graph
from graph.cluster import sorted, unique


import pattern
import pattern.web
import pattern.text
import pattern.text.en
en = pattern.text.en

import textblob

stopimport = time.time()
print("perception import pattern: %.3f" % (stopimport-startimport,)  )

</t>
<t tx="karstenw.20230316191528.10">def cache(key, value, ext='.json'):
    if ext=='.json':
        return open(_path(key,ext), "w").write(value)
    return open(_path(key,ext), "wb").write(value)


</t>
<t tx="karstenw.20230316191528.100">class compare_concepts(list):
    
    @others
</t>
<t tx="karstenw.20230316191528.101">def __init__(self, relation, cached=True, n=10):
    """ Comparative search heuristic in the form of:
        concept1 is-more-important-than concept2.
    Returns a list of (concept1, concept2)-tuples.
    Suggested patterns:
        "is-more-important-than", "is-bigger-than", "is-the-new", ...
    Requires the Web, Linguistics and Graph libraries.
    T. De Smedt, F. De Bleser
    """
    matches = search_match_parse(
        "\"" + relation.replace("-"," ") + "\"",
        "NN " + relation.replace("-"," ") + " (a) (an) (JJ) NN",
        lambda chunk_: (clean(chunk_[0][0]), clean(chunk_[-1][0])), # A is bigger than B --&gt; (A, B)
        # service="yahoo",
        service="google",
        cached=cached, 
        n=n
    )
    self.relation = relation.replace(" ","-")
    list.__init__(self, matches)
                    
</t>
<t tx="karstenw.20230316191528.102">def graph(self):
    """ Returns a graph with edges connecting concepts.
    Different unconnected clusters will be present in the graph.
    """
    try: graph._ctx = _ctx
    except:
        pass
    g = graph.create()
    for a, b in self:
        e = g.edge(b, a)
        if not e:
            e = g.add_edge(b, a)
        else:
            e.weight += 1
        if e: e.relation = self.relation.replace(" ", "-")
    style(g, relation=False)
    return g

</t>
<t tx="karstenw.20230316191528.103">def rank(self, graph=None):
    """ Returns a list of (concept1, [concept2, ...])-tuples sorted by weight.
    The weight is calculated according to nodes' eigenvalue in a graph.
    Be aware that tuples with the same weight may shuffle.
    For example:
    cmp = perception.compare_objects("is more important than")
    for concept1, comparisons in cmp.rank():
        for concept2 in comparisons:
            print concept1, cmp.comparison, concept2
    &gt;&gt;&gt; life is more important than money
    &gt;&gt;&gt; life is more important than kindness
    &gt;&gt;&gt; ...
    """
    # Nodes with the highest weight are at the front of the list.
    # eigensort = lambda graph, id1, id2: (graph.node(id1).weight &lt; graph.node(id2).weight)*2-1
    
    # sortlist(thelist, thecompare)
    if graph == None:
        graph = self.graph()
    r = []

    def linksort( a, b ):
        w1, w2 = graph.node(a).weight, graph.node(b).weight
        if w1 &gt; w2:
            return 1
        elif w1 &lt; w2:
            return -1
        return 0

    for node in graph.nodes:
        # Get the id's of all the nodes that point to this node.
        # Sort them according to weight.
        links = [n.id for n in node.links if node.links.edge(n).node1 == n]
        
        # links.sort(cmp=lambda a, b: eigensort(graph, a, b))
        sortlist(links, linksort)
        r.append((node.id, links))

    # r.sort(cmp=lambda a, b: eigensort(graph, a[0], b[0]))
    sortlist(r, linksort)
    r.reverse()
    return r

</t>
<t tx="karstenw.20230316191528.104">def suggest_comparisons(concept1, concept2, cached=True):
    """ With the reverse logic we can find relations between concepts.
    """
    matches = search_match_parse(
        "\"" + concept1 + " is * than " + concept2 + "\"",
        "is (*) * than",
        lambda chunk_: " ".join(x[0] for x in chunk_[1:-1]), # A is bigger than B --&gt; bigger
        service="google", 
        cached=cached
    )
    return count(matches)

</t>
<t tx="karstenw.20230316191528.11">def cached(key, ext='.json'):
    if ext == '.json':
        return open(_path(key, ext=ext)).read()
    return open(_path(key, ext=ext), 'rb').read()

</t>
<t tx="karstenw.20230316191528.12">def in_cache(key, ext=".json"):
    return os.path.exists(_path(key, ext))


</t>
<t tx="karstenw.20230316191528.13">def clear_cache():
    for f in glob.glob(os.path.join(CACHE, "*")):
        os.unlink(f)


</t>
<t tx="karstenw.20230316191528.14">#def is_robot(author):
#    return author == "robots@nodebox.net"

</t>
<t tx="karstenw.20230316191528.15">def normalize( s ):
    """ Returns lowercase version of string with accents removed.
    Used in Rules.disambiguate() to get the correct cluster root.
    """
    accents = [
        ("á|ä|â|å|à", "a"), 
        ("é|ë|ê|è", "e"), 
        ("í|ï|î|ì", "i"), 
        ("ó|ö|ô|ø|ò", "o"), 
        ("ú|ü|û|ù", "u"), 
        ("ÿ|ý", "y"), 
        ("š", "s"), 
        ("ç", "c"), 
        ("ñ", "n")
    ]
    s  = s.lower()
    for a, b in accents:
        s = sub(a, b, s)
    return s

</t>
<t tx="karstenw.20230316191528.16">class InternetError(BaseException):
    pass

</t>
<t tx="karstenw.20230316191528.17">class Rule:
    
    @others
</t>
<t tx="karstenw.20230316191528.18">def __init__(self, concept1, relation, concept2, lang1="en", lang2="en", context=None, weight=1.0,
                   author=None, date=None):
    """ A rule from the NodeBox Perception database,
    e.g. 'cat' is-a 'predator' in the 'nature' context.
    """
    self.concept1 = concept1
    self.lang1    = lang1
    self.relation = relation
    self.concept2 = concept2
    self.lang2    = lang2
    self.context  = context
    self.weight   = weight
    #self.author   = author
    #self.date     = date

</t>
<t tx="karstenw.20230316191528.19">def __repr__(self):
    s = self.concept1 + " " + self.relation + " " +self.concept2
    if self.context != None:
        s += " (" + self.context + ")"
    return s

</t>
<t tx="karstenw.20230316191528.2">def cmp_to_key(mycmp):
    'Convert a cmp= function into a key= function'
    class K:
        def __init__(self, obj, *args):
            self.obj = obj
        def __lt__(self, other):
            return mycmp(self.obj, other.obj) &lt; 0
        def __gt__(self, other):
            return mycmp(self.obj, other.obj) &gt; 0
        def __eq__(self, other):
            return mycmp(self.obj, other.obj) == 0
        def __le__(self, other):
            return mycmp(self.obj, other.obj) &lt;= 0
        def __ge__(self, other):
            return mycmp(self.obj, other.obj) &gt;= 0
        def __ne__(self, other):
            return mycmp(self.obj, other.obj) != 0
    return K


</t>
<t tx="karstenw.20230316191528.20">class Rules(list):

    @others
</t>
<t tx="karstenw.20230316191528.21">def _concepts(self):
    """ Returns a dictionary of concepts to number of occurences in the ruleset.
    """
    rank = {}
    for rule in self:
        if not rule.concept1 in rank:
            rank[rule.concept1] = 0
        if not rule.concept2 in rank:
            rank[rule.concept2] = 0
        rank[rule.concept1] += 1
        rank[rule.concept2] += 1
    return rank
    
concepts = property(_concepts)

</t>
<t tx="karstenw.20230316191528.22">def disambiguate(self, root=None):
    """ Disambiguate case-sensitive concepts (e.g. "god" or "God").
    Opt for what occurs most in the ruleset, this avoids unconnected graphs.
    The selected case for the given root concept is returned.
    """ 
    if root == None:
        return root
    self._root = root

    def _vote(concept, rank):
        lower = concept.lower()
        if lower in rank and rank[lower] &gt;= rank[concept]:
            return lower
        if lower == self._root:
            self._root = concept
        return concept

    rank = self.concepts
    for rule in self:
        rule.concept1 = _vote(rule.concept1, rank)
        rule.concept2 = _vote(rule.concept2, rank)

    # A search for "schrodinger" yields a cluster with "Schrödinger" as root.
    if self._root not in rank:
        for concept in rank.keys():
            if normalize(concept) == self._root.lower():
                self._root = concept
                break
    return self._root


</t>
<t tx="karstenw.20230316191528.24">def has_rule(node1, relation, node2=None, direct=True, reversed=False):
    """ Returns True if the edge between node1 and node2 is the given relation type.
    If node2 is not given, searches all linked nodes for the relation type.
    If direct is False, does a node1.con_reach(node2) search.
    """
    if node2 == None:
        # Check all connected nodes.
        for node2 in node1.links:
            if node1.has_rule(relation, node2, direct, reversed): 
                return True
        return False
    if isinstance(node2, str):
        node2 = node1.graph[node2]
    if reversed:
        node1, node2 = node2, node1
    if not direct:
        # If both nodes are the same, they can reach each other.
        # However, the given relation still needs to exist before we return True.
        # This is a special case but not impossible: recusion is-part-of recursion.
        if node1 == node2 \
        and ((not node2 in node1.links) \
        or (not node1.links.edge(node2).relation == relation)):
            return False
        # Check nodes connected to nodes with the given relation.
        return node1.can_reach(node2, 
            traversable=lambda n, e: n == e.node1 and e.relation == relation)
    # Assert that given relation goes from n1 directly to n2.
    if  node1 in node2.links \
    and node2.links.edge(node1).node1 == node1 \
    and node2.links.edge(node1).relation == relation:
        return True
    else:
        return False
    
graph.node.has_rule = has_rule

</t>
<t tx="karstenw.20230316191528.25">def is_a(node1, node2, direct=True)           : return node1.has_rule("is-a",           node2, direct)
def is_part_of(node1, node2, direct=True)     : return node1.has_rule("is-part-of",     node2, direct)
def is_opposite_of(node1, node2, direct=True) : return node1.has_rule("is-opposite-of", node2, direct)
def is_property_of(node1, node2, direct=True) : return node1.has_rule("is-property-of", node2, direct)
def is_related_to(node1, node2, direct=True)  : return node1.has_rule("is-related-to",  node2, direct)
def is_same_as(node1, node2, direct=True)     : return node1.has_rule("is-same-as",     node2, direct)
def is_effect_of(node1, node2, direct=True)   : return node1.has_rule("is-effect-of",   node2, direct)

def has_specific(node1, node2, direct=True)   : return node2.has_rule("is-a",           node1, direct)
def has_part(node1, node2, direct=True)       : return node2.has_rule("is-part-of",     node1, direct)
def has_property(node1, node2, direct=True)   : return node2.has_rule("is-property-of", node1, direct)
def has_effect(node1, node2, direct=True)     : return node2.has_rule("is-effect_of",   node1, direct)

graph.node.is_a           = is_a
graph.node.is_part_of     = is_part_of
graph.node.is_opposite_of = is_opposite_of
graph.node.is_property_of = is_property_of
graph.node.is_related_to  = is_related_to
graph.node.is_same_as     = is_same_as
graph.node.is_effect_of   = is_effect_of

graph.node.has_specific   = has_specific
graph.node.has_part       = has_part
graph.node.has_property   = has_property
graph.node.has_effect     = has_effect 

def is_property(node) : return node.has_rule("is-property-of")
def is_related(node)  : return node.has_rule("is-related-to")
def is_effect(node)   : return node.has_rule("is-effect-of")

graph.node.is_property = property(is_property)
graph.node.is_related  = property(is_related)
graph.node.is_effect   = property(is_effect)

</t>
<t tx="karstenw.20230316191528.3">def sortlist(thelist, thecompare):
    if py3:
        sortkeyfunction = cmp_to_key( thecompare )
        thelist.sort( key=sortkeyfunction )
    else:
        thelist.sort( thecompare )


</t>
<t tx="karstenw.20230316191528.39">def enumerate_rules(node, relation, depth=1, reversed=False):
    """ Lists all nodes involving edges of the given relation.
    With reversed=False, returns relations FROM the node: tree -&gt; tree is-a organism -&gt; organism
    With reversed=True, returns relations TO the node: tree -&gt; evergreen is-a tree -&gt; evergreen
    """
    if reversed:
        f = lambda a, b: b.has_rule(relation, a)
    else:
        f = lambda a, b: a.has_rule(relation, b)
    nodes = [n for n in node.links if f(node, n)]
    if depth &gt; 1:
        for n in nodes:
            nodes.extend(n.enumerate_rules(relation, depth-1, reversed))
    return unique(nodes)

graph.node.enumerate_rules = enumerate_rules

</t>
<t tx="karstenw.20230316191528.4">def nodecompare( a, b):
    if a[0] &gt; b[0]:
        return 1
    elif a[0] &lt; b[0]:
        return -1
    return 0


</t>
<t tx="karstenw.20230316191528.40">def specific(node, depth=1)     : return enumerate_rules(node, "is-a" ,          depth, reversed=True)
def parts(node, depth=1)        : return enumerate_rules(node, "is-part-of" ,    depth, reversed=True)
def opposites(node, depth=1)    : return enumerate_rules(node, "is-opposite-of", depth)
def properties(node, depth=1)   : return enumerate_rules(node, "is-property-of", depth, reversed=True)
def associations(node, depth=1) : return enumerate_rules(node, "is-related-to",  depth)
def aliases(node, depth=1)      : return enumerate_rules(node, "is-same-as",     depth)
def effects(node, depth=1)      : return enumerate_rules(node, "is-effect-of",   depth, reversed=True)

def type(node, depth=1)         : return enumerate_rules(node, "is-a",           depth)
def whole(node, depth=1)        : return enumerate_rules(node, "is-part-of",     depth)
def objects(node, depth=1)      : return enumerate_rules(node, "is-property-of", depth)
def causes(node, depth=1)       : return enumerate_rules(node, "is-effect-of",   depth)

graph.node.hypernyms    = graph.node.type         = type
graph.node.hyponyms     = graph.node.specific     = specific
graph.node.holonyms     = graph.node.whole        = whole
graph.node.meronyms     = graph.node.parts        = parts
graph.node.antonyms     = graph.node.opposites    = opposites
graph.node.objects                                = objects
graph.node.perceptonyms = graph.node.properties   = properties
graph.node.relations    = graph.node.associations = associations
graph.node.synonyms     = graph.node.aliases      = aliases
graph.node.effects                                = effects
graph.node.causes                                 = causes

</t>
<t tx="karstenw.20230316191528.51">def style(graph, relation=True):
    """ Apply styling to match the online Perception module.
    """
    try: __ctx = _ctx
    except:
        return
    graph.styles.background     = _ctx.color(0.36, 0.36, 0.34)
    graph.styles.root.text      = _ctx.color(1)
    graph.styles.dark.fill      = _ctx.color(0, 0.2)
    graph.styles.important.fill = _ctx.color(0, 0.2)
    if relation:
        # Associative nodes are green.
        s = graph.styles.create("related")
        s.fill = _ctx.color(0.5, 0.7, 0.1, 0.6)
        graph.styles.guide.append("related", lambda g, n: n.is_related)
        # Properties are blue.
        s = graph.styles.create("property")
        s.fill = _ctx.color(0.25, 0.5, 0.7, 0.7)
        graph.styles.guide.append("property", lambda g, n: n.is_property)
        # Ensure that the root node is styled last.
        graph.styles.guide.order.remove("root")
        graph.styles.guide.order += ["related", "property", "root"]

# Edge weight increases as more users describe the same rule.
VOTE = 0.05
      
</t>
<t tx="karstenw.20230316191528.52">def add_rule(graph, concept1, relation, concept2, context="", author="", 
             weight=0.5, length=1.0, label=""):
    """ Creates an edge between node1 and node2 of the given relation type and context.
    Multiple definitions of the same rule (e.g. by different users) increases the edge's weight.
    Multiple definitions differing in relation or context are currently ignored
    (e.g. a cookie can't be food and be part of food at the same time).
    """
    if relation == "is-opposite-of":
        # is-opposite-of edges are not interesting when looking for a shortest path,
        # because they represent a reversal in logic.
        weight = 0.25

    # switch to conceptnet
    #if author == "":
    #    # Discourage rules from anonymous authors.
    #    weight -= 0.25
    #if is_robot(author):
    #    # Robots score less than people.
    #    weight -= 0.25

    e = graph.edge(concept1, concept2)
    if e and e.relation == relation:# and e.context == context:
        v = VOTE
        #if is_robot(author):
        #    v *= 0.5
        e.weight += v
    else:
        e = graph.add_edge(concept1, concept2, weight, length, label)
        if e:
            e.relation = relation
            e.context  = context
            # e.author   = author
    return e
    
graph.graph.add_rule = add_rule

</t>
<t tx="karstenw.20230316191528.53">def cluster(concept, relation=None, context=None, author=None, depth=2, maxedges=None, labeled=False, 
            wait=15):
    """ Returns the given Perception query as a graph of connected concept nodes.
    """
    try:
        graph._ctx = _ctx
    except:
        pass
    rules = query(concept, relation, context, depth, maxedges, wait)

    concept = rules.disambiguate(concept)
    g = graph.create()
    style(g)
    if concept != None:
        g.add_node(concept, root=True)
    for rule in rules:
        e = g.add_rule(
            rule.concept1, 
            rule.relation, 
            rule.concept2,
            rule.context,
            "", #rule.author,
            weight = 0.5 + VOTE*(rule.weight-1)
        )
        if e and labeled:
            e.label = rule.relation
    return g

</t>
<t tx="karstenw.20230316191528.54">def proper_nouns(graph):
    return [n for n in graph.nodes if n.id != n.id.lower()]

</t>
<t tx="karstenw.20230316191528.55">def proper_leaves(graph):
    proper_nouns = graph.proper_nouns
    return [n for n in graph.leaves if n in proper_nouns]
    
graph.graph.proper_nouns = proper_nouns
graph.graph.proper_leaves = proper_leaves

</t>
<t tx="karstenw.20230316191528.56">def neighbors(node, nodes=None, distance=2, heuristic=None):
    """ Returns a selection of nearby nodes from the given list.
    A node must be reachable by a shortest path of the maximum given distance.
    A heuristic can furthermore encourage or discourage specific waypoints.
    """
    def _is_nearby(n):
        path = node.graph.shortest_path(node.id, n.id, heuristic)
        return path != None and len(path) &lt;= 1+distance
    if nodes == None:
        nodes = node.graph.nodes
    return filter(lambda n: _is_nearby(n), nodes)

graph.node.neighbors = graph.node.neighbours = neighbors

</t>
<t tx="karstenw.20230316191528.57">class cost(dict):
    """ A cost matrix for edge relations, used as graph.enumerate_rules() heuristic.
    """
    @others
heuristic = cost

</t>
<t tx="karstenw.20230316191528.58">def __init__(self, costs={}, graph=None):
    self.graph = graph
    for k, v in costs.items():
        self[k] = v
</t>
<t tx="karstenw.20230316191528.59">def tax(self, relation, cost):
    self[relation] = cost
</t>
<t tx="karstenw.20230316191528.6">def makeunicode(s, srcencoding="utf-8", normalizer="NFC"):
    try:
        if typ(s) not in (punicode, pstr):
            s = str( s )

        if typ(s) not in (punicode,):
            s = str(s, srcencoding)
    except TypeError:
        print( "makeunicode type conversion error" )
        print( "FAILED converting", typ(s), "to unicode" )
    s = unicodedata.normalize(normalizer, s)
    return s


</t>
<t tx="karstenw.20230316191528.60">def __call__(self, id1, id2):
    e = self.graph.edge(id1, id2)
    if e.relation in self:
        return self[e.relation]
    else:
        return 0
        
</t>
<t tx="karstenw.20230316191528.61">def graph_enumerate_rules(graph, relation, distance=2, heuristic=None, reversed=False):
    """ Returns nodes in the graph that have given outgoing relation.
    Nodes directly connected to the root are at the beginning of the list,
    followed by nodes sorted by weight (eigenvalue).
    The distance of the shortest path between the root and the node is limited,
    and a heuristic for finding shortest paths can be passed.
    """
    nodes = []
    if graph.root:
        nodes = graph.root.enumerate_rules(relation, 1, not reversed)
    nodes  = sorted(nodes, lambda a, b: (a.betweenness &lt; b.betweenness)*2-1) # XXX - use betweenness or eigenvalue?
    nodes += graph.nodes_by_eigenvalue(-1)
    nodes  = unique(nodes)
    nodes  = filter(lambda n: n.has_rule(relation, reversed=reversed), nodes)
    if graph.root and distance != None:
        if isinstance(heuristic, cost):
            heuristic.graph = graph
        nodes = graph.root.neighbors(nodes, distance, heuristic)
    return nodes
    
graph.graph.enumerate_rules = graph_enumerate_rules

# "Describe what the sun is like" -&gt; red, hot, bright, ...
</t>
<t tx="karstenw.20230316191528.62">def graph_properties(graph, distance=2, heuristic=None):
    """ Returns a list of important nodes that are property-of other nodes.
    By default, dislikes is-opposite-of paths.
    A "sun" node might have these direct properties: red, hot, bright, round.
    The algorithm will also find: slow, healthy, dangerous, blue, mysterious, 
    white, exotic, organic, passionate, chaotic, intense, fast, dry.
    """
    if not heuristic:
        heuristic=cost({"is-opposite-of": 10})
    return graph.enumerate_rules("is-property-of", distance, heuristic)

</t>
<t tx="karstenw.20230316191528.63"># "Specify concrete examples of a tree" -&gt; oak, linden, ...
def graph_specific(graph, proper=False, fringe=1):
    """ Returns a list of hyponym nodes in the fringe (all if None).
    """
    if fringe:
        nodes = graph.fringe(fringe)
    else:
        nodes = graph.nodes
    if graph.root:
        nodes = filter(lambda n: n.is_a(graph.root, direct=False), nodes)
    else:
        nodes = filter(lambda n: n.has_rule("is-a", direct=True), nodes)
    if proper:
        nodes = filter(lambda n: n in graph.proper_nouns(), nodes)
    return nodes  

</t>
<t tx="karstenw.20230316191528.64"># "Illustrate what wild is like" -&gt; landscape, anger, sea, rodeo, ...
def graph_objects(graph, distance=2, heuristic=None):
    """ Returns a list of tangible concepts.
    When the root is an object, returns a list of hyponym leaves. 
    For example, for a "tree" cluster: beech, birch, crapabble, dogwood, linden, ...
    When the root is a property, returns a list of objects.
    For example, for a "wild" cluster: landscape, forest, anger, sea, rodeo, ...
    """
    if not heuristic:
        heuristic=cost({"is-opposite-of": 10})
    if graph.root.is_property:
        nodes = graph.enumerate_rules("is-property-of", distance, heuristic, reversed=True)
        nodes = filter(lambda n: not n.is_property, nodes)
    else:
        nodes = graph.specific(proper=False)
    return nodes

graph.graph.perceptonyms = graph.graph.properties = graph_properties
graph.graph.hyponyms     = graph.graph.specific   = graph_specific
graph.graph.objects      = graph_objects

</t>
<t tx="karstenw.20230316191528.65">def taxonomy(concept, context, author=None, depth=4):
    """ Returns a graph containing only is-a edges.
    """
    return cluster(concept, "is-a", context, author, depth, wait=30)
    
hierarchy = taxonomy

</t>
<t tx="karstenw.20230316191528.66">class _range(dict):
    
    @others
range = _range()

</t>
<t tx="karstenw.20230316191528.67">def __init__(self):
    """ Creates a taxonomy from a given concept and filters hyponyms from it.
    For example: range.typeface -&gt; Times, Helvetica, Arial, Georgio, Verdana, ...
    For example: range.movie -&gt; Star Wars, Conan The Barbarian, ...
    The dictionary itself contains settings for how graph.specific() is called.
    """
    self.rules = {
                    #  concept     context   fringe proper 
        "animal"   : ("animal",   "nature",    3, False),
        "tree"     : ("tree",     "nature",    2, False),
        "flower"   : ("flower",   "nature",    2, False),
        "emotion"  : ("emotion",  "emotion",   2, False),
        "facial_expression" : ("facial expression", "emotion", 1, False),
        "movie"    : ("movie",    "media",     1, True),
        "font"     : ("typeface", "graphics",  2, True),
        "color"    : ("color",    "graphics",  4, False),
        "shape"    : ("shape",    "graphics",  4, False),
        "person"   : ("person",   "people",    4, True),
        "vehicle"  : ("vehicle",  "culture",   4, False),
        "building" : ("building", "culture",   2, False),
        "city"     : ("city",     "geography", 1, True),
        "country"  : ("country",  "geography", 1, True),
        "state"    : ("state",    "geography", 1, True),
    }

</t>
<t tx="karstenw.20230316191528.68">def append(self, name, concept, context, fringe=2, proper=False):
    self.rules[name] = (concept, context, fringe, proper)

</t>
<t tx="karstenw.20230316191528.69">def _hyponyms(self, concept, context=None, fringe=2, proper=False):
    g = taxonomy(concept, context)
    return sorted([n.id for n in g.hyponyms(proper, fringe=fringe)])

</t>
<t tx="karstenw.20230316191528.7">def hashFromString( s ):
    if typ( s ) in (punicode,):
        s = s.encode("utf-8")
    h = hashlib.sha1()
    h.update( s )
    return h.hexdigest()


</t>
<t tx="karstenw.20230316191528.70">def __getattr__(self, a):
    """ Each attribute behaves as a list of hyponym nodes.
    Attributes are expected to be singular nouns.
    Some of these are fine-tuned in the range, others are generic.
    """
    if a in self.rules:
        concept, context, fringe, proper = self.rules[a]
        return self._hyponyms(concept, context, fringe, proper)
    elif a == "properties":
        g = cluster(None, context="properties", depth=None, wait=30)
        return sorted(g.keys())
    elif a == "basic properties" \
      or a == "basic_properties":
          return basic_properties
    else:
        a = a.replace("_", " ")
        return self._hyponyms(a, None, 2, False)

</t>
<t tx="karstenw.20230316191528.71">def __call__(self, a):
    return self.__getattr__(a)

</t>
<t tx="karstenw.20230316191528.72">class _index(dict):
    
    @others
index = _index()

</t>
<t tx="karstenw.20230316191528.73">def __init__(self, name="properties"):
    self.name = name

</t>
<t tx="karstenw.20230316191528.74">def _get_name(self):
    return self._name
</t>
<t tx="karstenw.20230316191528.75">def _set_name(self, v):
    self._name = v
    self._file = os.path.join(INDEX, v)
    if os.path.exists(self._file):
        # dict.__init__(self, pickle.load(open(self._file,'rb')))
        f = open( self._file,'rb')
        s = f.read()
        f.close()
        d = pickle.loads( s, encoding="utf-8"  )
        dict.__init__(self, d )
    else:
        dict.__init__(self, {} )

name = property(_get_name, _set_name)

</t>
<t tx="karstenw.20230316191528.76">def build(self, name, concepts=[], heuristic=None):
    """ Caches the shortest paths between nodes in the given set.
    Retrieves the entire online Perception database as a graph.
    Creates a pickled index file.
    By supplying different sets of concepts, different index names
    and a different heuristic we can build custom indices.
    This proces can take some time (e.g. 30 minutes for 200 concepts on a
    MacBook Pro).
    """
    self._name = name
    g = cluster(None, depth=None, wait=600)
    if isinstance(heuristic, cost):
        heuristic.graph = g
    index = {}
    for i, concept1 in enumerate(concepts):
        for concept2 in concepts[i+1:]:
            path = g.shortest_path(concept1, concept2, heuristic)
            if concept1 not in index:
                index[concept1] = {}
            if concept2 not in index:
                index[concept2] = {}
            if path:
                index[concept1][concept2] = path[1:-1]
                index[concept2][concept1] = list(reversed(path[1:-1]))
    f = open(os.path.join(INDEX, self.name), "w")
    pickle.dump(index, f)
    f.close()

</t>
<t tx="karstenw.20230316191528.77">def shortest_path(self, concept1, concept2):
    """ Returns the shortest path between the given concepts (or None).
    """
    if concept1 == concept2: 
        return [concept2]
    if concept1 in self and \
       concept2 in self[concept1]:
        return [concept1] + self[concept1][concept2] + [concept2]

</t>
<t tx="karstenw.20230316191528.78">def nearest(self, root, concepts):
    """ Returns the concept from the list of concepts that is nearest to the root.
    """
    candidate, distance = None, 1000
    for concept in concepts:
        path = self.shortest_path(root, concept)
        if path and len(path) &lt; distance:
            candidate, distance = concept, len(path)
    return candidate
    
</t>
<t tx="karstenw.20230316191528.79">def sort_by_distance(self, root, concepts):
    """ Returns the list of concepts sorted by distance from root.
    """
    def _cmp(a, b):
        p1 = self.shortest_path(root, a)
        p2 = self.shortest_path(root, b)
        if p1 == None: return  1
        if p2 == None: return -1
        return len(p1) - len(p2)
    return sorted(concepts, _cmp)
    
</t>
<t tx="karstenw.20230316191528.8"># "_range" is the name of a singleton class in the library.
# "range" is the name of the class instance.
def range_(start, stop=None, step=1):
    if stop is None: 
        stop, start = start, 0
    cur = start
    while cur &lt; stop:
        yield cur
        cur += step


</t>
<t tx="karstenw.20230316191528.80">def _build_properties_index(): 
    index.build(
        "properties",
        range.properties,
        cost({"is-property-of" : -0.25,
              "is-same-as"     : -0.5,
              "is-opposite-of" : 10})
    )

</t>
<t tx="karstenw.20230316191528.81">def add_method(name, method):
    setattr(graph.graph, name, method)
graph.add_method = add_method

</t>
<t tx="karstenw.20230316191528.82">def score(list, m=0.5):
    """ Returns the sum of the list, where each item is increasingly dampened.
    """
    n = 0
    for x in list:
        n += x*m
        m = m**2
    return n

# Cache the results from concept cluster graphs.
_SOLVER_CACHE = {}

</t>
<t tx="karstenw.20230316191528.83">class IndexError(Exception): pass
</t>
<t tx="karstenw.20230316191528.84">class MethodError(Exception): pass

</t>
<t tx="karstenw.20230316191528.85">class analysis(dict): pass

</t>
<t tx="karstenw.20230316191528.86">class _solver:

    @others
solver = _solver()

</t>
<t tx="karstenw.20230316191528.87">def __init__(self, index="properties", method="properties"):
    self.index = index
    self.method = method
    self.analysis = analysis()
    self.m = 0.5 # score dampener
    
</t>
<t tx="karstenw.20230316191528.88">def _prepare(self):
    if index.name != self.index:
        index.name = self.index
        _SOLVER_CACHE.clear()
    if len(index) == 0:
        raise IndexError( "index '"+ index.name +
            "' to use with solver is empty or non-existent" )
    if not hasattr(graph.graph, self.method):
        raise MethodError( "'"+self.method+\
            "' is not an existing cluster method the solver can call" )
    self.analysis = analysis()

</t>
<t tx="karstenw.20230316191528.89">def _retrieve(self, concept, depth=3, cached=True):
    
    """ Creates a concept cluster graph and returns the results from self.method.
    For example =&gt; cluster("sun").properties()
    The results are cached for faster retrieval.
    """
    
    if not (cached and concept in _SOLVER_CACHE):
        g = cluster(concept, depth=depth)
        _SOLVER_CACHE[concept] = [n.id for n in getattr(g, self.method)()]
    return _SOLVER_CACHE[concept]

</t>
<t tx="karstenw.20230316191528.9">def _path(key, ext='.json'):
    h = hashFromString(key)
    return os.path.join(CACHE, h + ext)


</t>
<t tx="karstenw.20230316191528.90">def sort_by_relevance(self, root, concepts, threshold=0, weighted=False):
    
    """ Returns the list of concepts sorted by relevance to the root [property].
    
    1) For each of the concepts, a Perception cluster is created.
    2) The cluster is analyzed for [properties] that best describe the
       concept (eigenvalue).
    3) A cached index of distances between all [properties] is used 
       to decide which [properties] of which concept are closest to the given
       query (Dijkstra).
    4) A list of winning concepts is returned (closest first).
    
    For example: painful &lt;-&gt; range.emotion = shame, envy, pride, anger, jealousy, sadness, fear, anxiety, ...
    
    The word [properties] can be replaced with another type of concept,
    as long as it is a graph method the solver can call,
    e.g. graph.properties(), graph.specific(), ...
    
    The threshold defines the minimum amount of [properties] a concept must have.
    
    To get absurd results, the returned list can be reversed.
    
    """

    self._prepare()
    candidates = []
    for concept in concepts:
        # Create a cluster of related concepts.
        # Find the [properties] that best describe the concept.
        # Find paths to the root for each [property]. 
        # The length of the paths at the start of the list is important.
        if concept == root:
            S = [[] for i in range_(100)]
        else:
            S = self._retrieve(concept, depth=3, cached=True)
            S = [index.shortest_path(n, root) for n in S]
            S = [path for path in S if path != None]
        if len(S) &gt; threshold: # We can filter out poorly described concepts here.
            self.analysis[concept] = S # Cache the paths for later analysis.
            S = [len(path) for path in S]
            candidates.append((concept, S))
    
    if len(candidates) == 0: 
        return []
    
    # Find the concept with the least [properties]:
    # this is the maximum amount of paths we can compare to find the best candidate.
    n = min([len(paths) for concept, paths in candidates])
    candidates = [(score(paths[:n], self.m), concept)  for concept, paths in candidates]
    # Best candidates will be at the front of the list.
    
    sortlist(candidates, nodecompare)
    # candidates.sort()
    if not weighted:
        candidates = [concept for x, concept in candidates]
    
    # The analysis dictionary contains an overview of the thought process.
    # How each concept travels to the given root.
    for key in self.analysis:
        self.analysis[key] = self.analysis[key][:n]
    
    return candidates

find = sort_by_relevance

</t>
<t tx="karstenw.20230316191528.91">class _analogy:
    
    @others
analogy = _analogy()

</t>
<t tx="karstenw.20230316191528.92">def __init__(self):
    self.analysis = analysis()
    self.depth = 20 # how many object properties to analyze?
    self.m = 0.9 # score dampener

</t>
<t tx="karstenw.20230316191528.93">def _best_first(self, list, candidates=[]):
    """ Returns a list copy with items that appear in candidates at the head.
    We use this to implement common sense. 
    Example: assume we want a color analogy for "water".
    - Properties for water: cool, clean, wet, transparent, slow, ..., blue, ...
    - Color concepts: black, blue, green, red, yellow, ...
    - Obviously, "blue" makes an excellent candidate. 
    - However, the "blue" property is too far down the list to score highly.
    - Therefore, we sort the water properties to available colors, putting blue
      at the front.
    """
    list = [item for item in list]
    candidates = [item for item in list if item in candidates]
    for item in reversed(candidates):
        if item in list:
            list.remove(item)
            list.insert(0, item)
    return list

</t>
<t tx="karstenw.20230316191528.94">def __call__(self, object, concepts, properties=[], threshold=0, weighted=False):
    
    """ For each [property] of the given object, sort concepts by relevance.
    An object is a concept that has properties: sea, Darwin, church, sword, cat, ...
    Returns the list of concepts sorted by the relevance score sum.
    
    For example:
    sword &lt;=&gt; range.animal = hedgehog, scorpion, bee, cat, cheetah, cougar, ...
    
    """
    
    S = analysis([(concept, {}) for concept in concepts])
    p = properties + solver._retrieve(object)
    p = self._best_first(p, candidates=concepts)

    for i, property in enumerate(p[:self.depth]):
        
        # The score for each concept is the sum of the length of the paths 
        # from the given property to each of the concept's properties,
        # dampened by order.
        A = solver.find(str(property), concepts, threshold)
        A = solver.analysis
        for concept in A:
            A[concept] = [len(path) for path in A[concept]]
            A[concept] = score(A[concept], self.m)
        # Keep track of the scores of all the properties of the given object.
        for concept in A:
            S[concept][property] = A[concept] * self.m**i
    
        # The solver may not be able to find a path to each concept.
        # We then give it the worst possible score.
        if len(A) &gt; 0:
            m = max(A.values()) + 0.00001
            for concept in S:
                if concept not in A:
                    S[concept][property] = m
    
    self.analysis = S
    
    results = [(sum(S[concept].values()), concept) for concept in S]
    sortlist(results, nodecompare)
    # results.sort()
    if not weighted:
        results = [concept for n, concept in results]
    return results

map_object = __call__

</t>
<t tx="karstenw.20230316191528.95">def search_match_parse(query, pattern_, parse=lambda x: x, service="google", cached=True, n=2,):
    """ Parses words from search engine queries that match a given syntactic pattern.
    query   : a Google/Yahoo query. Google queries can include * wildcards. 
    pattern : an en.sentence.find() pattern, e.g. as big as * NN
    parse   : a function that filters data from a tagged sentence.
    """

    Google = pattern.web.Google
    Yahoo = pattern.web.Yahoo
    Bing = pattern.web.Bing
    asynchronous =  pattern.web.asynchronous
    plaintext = pattern.web.plaintext
    
    matches = []
    rawresults = []

    if 1:
        print( "search_match_parse query:", query )
        print( "search_match_parse pattern_:", pattern_ )
    if service == "google":
        n = min(n, 4)
        engine = Google(license=None, language="en")
    elif service == "yahoo" :
        n = min(n, 10)
        engine = Yahoo(license=None, language="en")
    elif service == "bing" :
        n = min(n, 10)
        engine = Bing(license=None, language="en")

    collect = []
    for i in range_(n):
        if service == "google":
            # search = web.google.search(query, start=i*4, cached=cached)
            for result in engine.search(query, start=i, count=10): #, type=SEARCH, cached=True):
                collect.append( result )

        elif service == "yahoo":
            # search = web.yahoo.search(query, cached=cached, start=i*100, count=100)
            # __init__(self, license=None, throttle=0.5, language=None):
            #     SearchEngine.__init__(self, license or YAHOO_LICENSE, throttle, language)
            for result in engine.search(query, start=i, count=10): #, type=SEARCH, cached=True):
                collect.append( result )

        elif service == "bing":
            request = asynchronous(engine.search, query, start=1, count=100) #, type=SEARCH, timeout=10)
            while not request.done:
                time.sleep(0.1)
                print(".", end="")
            if request.error:
                raise request.error
            for result in request.value:
                collect.append( result )

    pdb.set_trace()
    
    for result in collect:
        if result.text:
            # result.description = result.description.replace(",",", ").replace("  "," ")
            #match = en.sentence.find(result.description.lower(), pattern_)
            #if len(match) &gt; 0 and len(match[0]) &gt; 0:
            #    x = parse(match[0])
            #    matches.append(x)
            if 1:
                pp( result )
            if pattern_ in result.text:
                matches.append(result.text)
    return matches

</t>
<t tx="karstenw.20230316191528.96">def count(list):
    """ Returns a dictionary with the list items as keys and their count as values.
    """
    d = {}
    for v in list: d[v]  = 0
    for v in list: d[v] += 1
    return d

</t>
<t tx="karstenw.20230316191528.97">def clean(word):
    word = word.strip(",;:.?!'`\"-[()]")
    word = word.strip(u"‘“")
    if word[-2:] in ("'s", "’s"):
        word = word[:-2]
    if 0:
        if word.endswith( "'s"):
            word = word[:-2]
        if word.endswith(u"’s"):
            word = word[:-2]
    return word.strip()

</t>
<t tx="karstenw.20230316191528.98"># NOT WORKING
def suggest_properties(noun, cached=True):
    """ Learning to Understand Figurative Language: 
        From Similes to Metaphors to Irony,
        Tony Veale, Yanfen Hao
        http://afflatus.ucd.ie/Papers/LearningFigurative_CogSci07.pdf
    
    Uses simile to retrieve properties of nouns.
    For example: troll -&gt; "as ugly as a troll" -&gt; ugly is-property-of troll.
    Requires the Web and Linguistics libraries.
    Returns a dictionary of properties linked to count.
    """
    # import en
    # noun = en.noun.article(noun)
    noun = en.inflect.referenced( noun )
    if 1:
        print("suggest_properties noun:", noun)
    matches = search_match_parse(
        "\"as * as " + noun + "\"",
        "as * as " + noun,
        lambda chunk_: clean(chunk_[1][0]), # as A as a house -&gt; A
        service="google", 
        cached=cached
    )
    if 1:
        print("suggest_properties matches:", matches)
    matches = filter(lambda word: word not in ("well", "much"), matches)
    if 1:
        print("suggest_properties filtered matches:", matches)
    return count(matches)

</t>
<t tx="karstenw.20230316191528.99">def suggest_objects(adjective, cached=True):
    """ With the reverse logic we can find objects.
    For example: blue -&gt; as blue as the sky -&gt; sky has-property blue.
    """
    # import en
    def _parse(chunk_):
        # as deep as the oceans -&gt; ocean
        noun = clean(chunk_[-1][0])
        if chunk_[-2][0] == "the" and en.is_noun(en.noun.singular(noun)):
            return en.noun.singular(noun)
        return noun
    matches = []
    for adverb in ("a", "an", "the"):
        matches += search_match_parse(
            "\"as " + adjective + " as " + adverb + " *\"",
            "as " + adjective + " as * NN",
            _parse,
            service="google", 
            cached=cached
        )
    matches = filter(lambda word: len(word) &gt; 1 and word not in ("****"), matches)
    return count(matches)

</t>
<t tx="karstenw.20230316191631.1"></t>
<t tx="karstenw.20230316191639.1"># py3 stuff
py3 = False
try:
    unicode('')
    punicode = unicode
    pstr = str
    punichr = unichr
except NameError:
    punicode = str
    pstr = bytes
    py3 = True
    punichr = chr
    long = int


</t>
<t tx="karstenw.20230316191737.1">#### BASIC PROPERTIES ################################################################################
# Those we think can easily be translated visually.

basic_properties = [
    "angular",  "round",      "large",    "small",
    "long",     "short",      "bright",   "dark",
    "calm",     "wild",       "chaotic",  "structured",
    "clean",    "dirty",      "cold",     "hot",
    "cool",     "warm",       "sharp",    "soft",
    "complex",  "simple",     "deep",     "shallow",
    "dynamic",  "static",     "fast",     "slow",
    "fluid",    "solid",      "hard",     "soft",
    "heavy",    "light",      "loud",     "quiet",
    "natural",  "artificial", "old",      "new",
    "elegant",  "raw",        "strong",   "weak",
    "tangible", "abstract",   "thick",    "thin", "repetitive"
]

</t>
<t tx="karstenw.20230316191816.1">#### CACHE ###########################################################################################
# Caches the results returned from the NodeBox Perception API so queries run faster.

# TODO: Move to linguistics data
CACHE = os.path.abspath(os.path.join(os.path.dirname(__file__), "cache"))


</t>
<t tx="karstenw.20230316194255.1">#### CONCEPT CLUSTER ################################################################################# 
# Extends the Graph and Node objects for handling a semantic network of rules.

#--- NODE RULE ASSERTION -----------------------------------------------------------------------------

graph.edge.relation = None
graph.edge.context = None
graph.edge.author = None

</t>
<t tx="karstenw.20230316194456.1">#--- NODE RULE RETRIEVAL -----------------------------------------------------------------------------

</t>
<t tx="karstenw.20230316194540.1">#--- CLUSTER ----------------------------------------------------------------------------------------

</t>
<t tx="karstenw.20230316194605.1">#--- CLUSTER HEURISTICS ------------------------------------------------------------------------------

</t>
<t tx="karstenw.20230316194702.1">#### TAXONOMY ########################################################################################
# Taxonomies are used to find specific/concrete interpretations of a concept.

</t>
<t tx="karstenw.20230316194807.1">#### INDEX #############################################################################
# A cached index of shortest paths between concepts.
# You give it a list of concepts and it looks up all the paths between them,
# based on the rules in the Perception database.
# This is essential: we can't create a NodeBox visualization script for each and every
# concept.  We will later need to "solve" how to get from something undefined to
# something defined.

INDEX = os.path.join(os.path.dirname(__file__), "index")

</t>
<t tx="karstenw.20230316194837.1">#### SOLVER ############################################################################
# The solver finds the best match between a property and a range of concepts
# (e.g. creepy &lt;=&gt; flowers).
# To retrieve the creepiest flower, it analyzes the properties of each specific flower.
# For each of these properties, we calculate the shortest path to "creepy".
# Less important properties (further down the list) have less impact on the total score
# (factor m).
# XXX - how many paths does the human brain take into account (high m, low m?)
</t>
<t tx="karstenw.20230316194920.1">#--- ANALOGY ---------------------------------------------------------------------------
# Uses the solver to analyze multiple properties of a given object.
# This way we can map objects to a different context: 
# music styles to colors, people to animals, cars to geometric shapes, ...
# e.g. what kind of animal is George W. Bush?

</t>
<t tx="karstenw.20230316195011.1">#--- SIMILE ----------------------------------------------------------------------------

</t>
<t tx="karstenw.20230316195027.1">#--- COMPARATIVE -----------------------------------------------------------------------

</t>
<t tx="karstenw.20230316195056.1">########################################################################################

#solver.index = "properties"
#solver.method = "properties"
#print solver.sort_by_relevance("dark", range("emotion"))
#print solver.sort_by_relevance("bright", range("emotion"))

# BETA+
# - Implemented is-effect-of and has-effect rules.
# - Perception module has a right-click preferences menu for columns.
# - The solver's analysis is deeper (multiple properties per concept examined).
# - The solver uses caching for graph properties.
# - New analogy component.
# - graph.enumerate_rules() sorts according to betweenness instead of eigenvalue.

# BETA
# - query() and cluster() have a wait parameter.
# - Rules now have a weight.
# - search-match-parse, similes and comparatives.
# - Optimized index builder speed by removing reverse paths.
# - Rebuilt properties index.
# - Tested if numerical index paths instead of string index paths 
#   would shrink file size: no difference (due to pickle format?)


</t>
<t tx="karstenw.20230318130644.1">#### API #############################################################################################
# The query() command returns a list of rules from the online Perception database.
# Locally cached results are used whenever available.

AUTHOR = ""
</t>
<t tx="karstenw.20230318131229.1"></t>
<t tx="karstenw.20230329135420.1">@
def query_cn(concept, relation=None, context=None, depth=1, max=None, wait=2, lang="en"):
    
    """ Returns search results from Conceptnet.
    Retrieves a list of rules involving the given concept, relation and context.
    If depth is &gt; 1, returns a cluster of rules:
    - concepts connected to the given concept = depth 1
    - concepts connected to the depth 1 set = depth 2, etc.
    """

    if concept == None:
        return Rules()

    concept = concept.lower()
    concept = concept.replace(" ", "_")
    lang = lang.lower()
    url = make_url( concept, lang)
    # pdb.set_trace()

    response, data, pick = get_query_lang_concept( concept, lang, docache=True )
    if not 'error' in data:
        if pick==0:
            data, filecount = get_all_pages( data )
            cache( url, pickle.dumps( data ), ext='.pickle' )
    else:
        pp( data['error'] )
    return data


    
    edges = get_edge_nodes( data )
    
    rules = Rules()
    if not data:
        return rules
    
    for rule in response.split("\n"):
        rule = rule.split(",")
        concept1 = _parse(rule[0])
        relation = _parse(rule[1])
        concept2 = _parse(rule[2])
        context  = _parse(rule[3])
        weight   = float(_parse(rule[4]))
        # author   = _parse(rule[5])
        date     = _parse(rule[6])
        rules.append(Rule(concept1, relation, concept2, context, weight=weight )) #, author=author, date=date))
        
    return rules

</t>
<t tx="karstenw.20230329145218.1">@
def get_query_lang_concept( concept, language, docache=True ):
    """Retrieve the URL and cache the JSON result.
    """
    # print("get_query_lang_concept( %s, %s )" % (concept, language) )
    response = ""
    data = {}
    
    url = 'http://api.conceptnet.io/c/%s/%s' % (language,concept)
    found = False
    if docache:
        if in_cache( url, ext='.pickle'):
            print("CACHED PICKLE", concept)
            s = cached( url, ext='.pickle')
            data = pickle.loads( s )
            return "", data, 1
        elif in_cache( url ):
            print("CACHED JSON", concept)
            jsontext = cached( url )
            data = json.loads( jsontext )
            return "", data, 0

    response, data, err = download_url( url )
    if docache:
        cache(url, json.dumps(data), ext='.json')
    return response, data, 0


</t>
<t tx="karstenw.20230329150729.1">@
def get_egde_nodes( data ):
    result = []
    if 'edges' in data:
        result.extend( data['edges'] )
    return result


</t>
<t tx="karstenw.20230329150748.1">@
def get_next_page( data ):
    # dummy for now
    if not 'view' in data:
        print("get_next_page( data ) FINISHED 1.")
        return 1,data
    
    view = data['view']
    if not 'nextPage' in view:
        print("get_next_page( data ) FINISHED 2.")
        return 1,data
    
    cnconcept = view['nextPage']
    resp, newdata = get_query_cnconcept( cnconcept, docache=True )
    if 'paginatedProperty' in view:
        paginatedProperty = view['paginatedProperty']
        data[paginatedProperty].extend( newdata[paginatedProperty] )
        if 'nextPage' in newdata['view']:
            if 0: #resp != "":
                print("cur / next:", view['nextPage'], newdata['view']['nextPage'] )
            if view['nextPage'] != newdata['view']['nextPage']:
                view['nextPage'] = newdata['view']['nextPage']
                if 0: #resp != "":
                    print("get_next_page( data ) CONTINUED.")
                return 0,data
            else:
                if 0: #resp != "":
                    print("get_next_page( data ) FINISHED 3.")
                return 1,data
        else:
            view['lastPage'] = view.get('nextPage', "")
            view.pop('nextPage', "")
            if 0: #resp != "":
                print("get_next_page( data ) FINISHED 4.")
            return 1,data
    if resp != "":
        print("get_next_page( data ) FINISHED 5.")
    return 1,data


</t>
<t tx="karstenw.20230330100356.1">#### SEARCH-MATCH-PARSE ################################################################

</t>
<t tx="karstenw.20230330102024.1"></t>
<t tx="karstenw.20230330103714.1">@
def get_query_cnconcept( cnconcept, docache=True ):
    """Retrieve the conceptnet concept (/c/en/word ) and cache the JSON result.
    """
    response = ""
    data = {}
    
    url = 'http://api.conceptnet.io%s' % (cnconcept,)
    
    print("get_query_cnconcept( %s )" % (cnconcept,) )

    if docache and in_cache( url ):
        jsontext = cached( url )
        data = json.loads( jsontext )
        response = ""
        print("CACHED")
    else:
        response, data, err = download_url( url )
        if not 'error' in data:
            if docache:
                cache(url, json.dumps(data), ext='.json')
    return response, data


</t>
<t tx="karstenw.20230330104408.1">@
def get_all_pages( data ):
    # pdb.set_trace()
    i = 1
    if not 'edges' in data:
        pp( data )
        pdb.set_trace()
        print()
    print("#edges %i:" % (i,), len(data['edges']) )
    while True:
        finished, newdata = get_next_page( data )
        # print("get_all_pages -&gt; finished:", finished, i)
        i += 1
        data = newdata
        print("#edges %i:" % (i,), len(data['edges']) )
        if finished:
            break
    return data, i


</t>
<t tx="karstenw.20230330142517.1">@
def download_url( url ):
    response = ""
    data = {}
    err = ""
    # print("URL load:", url)
    try:
        # setdefaulttimeout(wait)
        response = requests.get( url )
        jsontext = response.json()
        time.sleep(0.9)
    except Exception as err:
        print("\ndownload_url() failed.")
        print(url)
        print(err)
        print()
        # raise InternetError( url + "\n" + e)
    return response, jsontext, err

</t>
<t tx="karstenw.20230330170426.1">@
def make_url( concept, language="" ):
    if language != "":
        return 'http://api.conceptnet.io/c/%s/%s' % (language,concept)
    return 'http://api.conceptnet.io%s' % (concept,)


</t>
<t tx="karstenw.20230403165828.1">def _newpath( key, ext='.json' ):
    h = hashFromString(key)
    foldername = h[:2]
    folder = os.path.join(CACHE, folder)
    if not os.path.exists( folder ):
        os.makedirs( folder )
    return os.path.join(CACHE, folder, h + ext)


</t>
<t tx="karstenw.20230403170027.1">@
def getoldwords():
    words = getpatternwords()
    # print(words)
    print("#words and concepts:", len(words))
    cachedc, totalc, words = scancache( words )
    print( "LOADED:  %i / %i" % (cachedc, totalc) )
    print( "REMAINING:", len(words) )
    for word in words:
        print()
        print(word)
        q = query_cn( word )


</t>
<t tx="karstenw.20230403170112.1"></t>
<t tx="karstenw.20230403170129.1">@
def getpatternwords():
    path = os.path.abspath("./doc/data/commonsense.tab")
    f = open(path, 'r')
    words = set()
    # pdb.set_trace()
    for line in f:
        line = line.rstrip( "\r\n" )
        items = line.split("\t")
        c1,rel,c2,ctx,w = items
        if float(w) &gt;= 1.0:
            words.add( c1.lower().strip() )
            words.add( c2.lower().strip() )
            if ctx.lower().strip():
                words.add( ctx.lower().strip() )
    return words


</t>
<t tx="karstenw.20230403170137.1">@
def scancache( concepts ):
    totalc = cachedc = 0
    result = set()
    for concept in concepts:
        totalc += 1
        concept = concept.lower()
        concept = concept.replace(" ", "_")
        url = make_url( concept, 'en')
        if in_cache( url, ext='.pickle'):
            cachedc += 1
        else:
            result.add( concept )
    return cachedc, totalc, result


</t>
<t tx="karstenw.20230403170145.1">@
def loadcache():
    allfiles = cachefiles( old=True )
    result = {}
    paths = []
    
    # pdb.set_trace()
    
    for path in allfiles:
        path = os.path.abspath( path )
        folder, filename = os.path.split( path )
        basename, ext = os.path.splitext( filename )
        
        if ext.lower() not in ('.pickle',):
            continue
        paths.append( path )
    
    
    fout = open("allrules.tab", 'w')
    keyout = open("allkeys.tab", 'w')
    
    okrelations = (
        "/r/RelatedTo /r/FormOf /r/IsA /r/PartOf /r/HasA "
        "/r/UsedFor /r/CapableOf /r/AtLocation /r/HasProperty "
        "/r/Synonym /r/Antonym /r/DistinctFrom /r/DerivedFrom "
        "/r/SymbolOf /r/DefinedAs /r/MannerOf /r/HasContext "
        "/r/SimilarTo /r/EtymologicallyRelatedTo /r/EtymologicallyDerivedFrom "
        "/r/MadeOf ").split()
    okrelations = tuple( okrelations )
    
    symmetricrelations = (
        "/r/RelatedTo /r/Synonym /r/Antonym /r/DistinctFrom /r/LocatedNear "
        "/r/SimilarTo /r/EtymologicallyRelatedTo ").split()
    symmetricrelations = tuple( symmetricrelations )
    
    template = "%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\n"
    fout.write( "allrulesid\t"
                "concept1\tconcept1label\tconcept1language\ttype1\tcontext1\t"
                "relation\t"
                "concept2\tconcept2label\tconcept2language\ttype2\tcontext2\t"
                "weight\n")
    keyout.write( "idallrules\n" )
    
    translationskips = doubles = 0
    
    # switches
    
    # skip translation /r/RelatedTo
    skipRelatedTranslations = True
    
    # 
    # filter and convert perception relations
    # 
    #
    
    longurls = {}
    i = 0
    
    uniquerules = {}
    
    
    for path in paths:
        f = open( path, 'rb' )
        data = pickle.load( f )
        f.close()
        key = data['@id']
        if key in result:
            pdb.set_trace()
            print( data )
            continue
        result[ key ] = []
        records = set()
        concepts = {}
        keyout.write( "%s\n" % key )
        for edge in data['edges']:
            rel = edge['rel']['@id']
            
            i += 1
            if i % 10000 == 0:
                sys.stdout.write('.')
                if i % 1000000 == 0:
                    sys.stdout.write('  %i\n' % (i,))
                    if i % 5000000 == 0:
                        sys.stdout.write('\n')
                sys.stdout.flush()

            if not rel in okrelations:
                continue

            # start = edge['start']
            startid = edge['start']['@id']
            startlabel = edge['start']['label']
            startlang = edge['start']['language']
            type1 = src1 = context1 = ""

            # end = edge['end']
            # end = edge['end']['@id']
            endid = edge['end']['@id']
            endlabel = edge['end']['label']
            endlang = edge['end']['language']
            type2 = src2 = context2 = ""
            
            recordkey = (startid,rel,endid)
            if recordkey not in uniquerules:
                uniquerules[recordkey] = 0
            uniquerules[recordkey] += 1
            
            w = str( round( edge['weight'], 5 ) )
            w = w.replace(".", ",")

            #
            # check for long conceptnet urls
            #
            # /c/LANG/CONCEPT/TYPE/SOURCE/CONTEXT

            n = startid.count("/")
            if n &gt; 3:
                # pdb.set_trace()
                items = startid.split('/')
                type1 = items[4]
                if n &gt; 4:
                    src1 = items[5]
                    if src1 not in ("jmdict", "wikt"):
                        longurl = '/'.join( ('', *items[5:]) )
                        if longurl not in longurls:
                            longurls[longurl] = 0
                        longurls[longurl] += 1
                if n &gt; 5:
                    if src1 not in ("jmdict", "wikt"):
                        context1 = longurl # items[6]
            
            n = endid.count("/")
            if n &gt; 3:
                # pdb.set_trace()
                items = endid.split('/')
                type1 = items[4]
                if n &gt; 4:
                    src2 = items[5]
                    if src2 not in ("jmdict", "wikt"):
                        longurl = '/'.join( ('', *items[5:]) )
                        if longurl not in longurls:
                            longurls[longurl] = 0
                        longurls[longurl] += 1
                if n &gt; 5:
                    if src2 not in ("jmdict", "wikt"):
                        context2 = longurl # items[6]
            
            # is key concept 1 or concept 2
            # mainlabel = 
            
            # check language inequality - only synonyms, 
            if skipRelatedTranslations and (startlang != endlang):
                if rel in ( '/r/RelatedTo', ):
                    translationskips += 1
                    continue
            
            recordkey = (startid,rel,endid)
            
            record = (  key,
                        startid,startlabel,startlang,type1,context1,
                        rel,
                        endid,endlabel,endlang,type2,context2,
                        w )
            
            # canonicalize - check doubles
            if recordkey in records:
                doubles += 1
                continue
            
            records.add( recordkey )
            
            fout.write( template % record )

    print("doubles:", doubles)
    print( "translationskips:", translationskips )
    # print("longurls:")
    
    
    # uniquerules
    uniqueout = open("alluniques.tab", 'w')
    uniqueout.write("idunique\tcount\n")
    template = "%s\t%i\n"
    
    # uniquerules[recordkey]
    total = 0
    for recordkey in uniquerules:
        count = uniquerules[recordkey]
        if int(count) &gt; 1:
            total += count
            uniqueout.write( template % (recordkey,count) )
    uniqueout.write("Summe\t%i\n" % (total,) )
    uniqueout.close()
    
    
    contextout = open("allcontexts.tab", 'w')
    contextout.write("idcontext\tcount\n")
    template = "%s\t%i\n"
    for longurl in longurls:
        count = longurls[longurl]
        contextout.write( template % (longurl,count) )
    contextout.close()
    # pp(longurls)
    keyout.close()
    fout.close()


</t>
<t tx="karstenw.20230403170157.1">@
def ask(name, lang="en"):
    q = query_cn( name, lang=lang )
    
    alledges = len(q.get('edges', []))
    langedges = 0
    for edge in q.get('edges', []):
        start = edge.get('start', False)
        end = edge.get('end', False)
        rel = edge.get('rel', False)
        if not start and end and rel:
            pdb.set_trace()
            print(start)
            print(rel)
            print(end)
            continue
        else:
            lg1 = start.get("language", "''")
            lb1 = start.get("label", start['@id'])
            lg2 = end.get("language", "''")
            lb2 = end.get("label", end['@id'])
            w = edge.get('weight', 0.1)
            rell = rel['label']
            if rell in ('ExternalURL',):
                continue
            if lg1 != lg2:
                # continue
                if lg1 not in ('de', 'en', 'fr', 'es', 'it', 'da', 'no', 'sv'):
                    continue
                if lg2 not in ('de', 'en', 'fr', 'es', 'it', 'da', 'no', 'sv'):
                    continue
            if w &lt; 1.5:
                continue
            t = "%s:%s  --%s--  %s:%s    %.3f" 
            print( t % (lg1,lb1,rell,lg2,lb2,w) )
            langedges += 1
    print("#edges: %i / %i" % (langedges,alledges) )
    return 

</t>
<t tx="karstenw.20230417105918.1">def query_cnr(concept, relation=None, context=None, depth=1, maxedges=0, wait=2, lang="en"):
    
    """ Returns search results from sqlite database.

    Retrieves a list of rules involving the given concept, relation and context.
    If depth is &gt; 1, returns a cluster of rules:
    - concepts connected to the given concept = depth 1
    - concepts connected to the depth 1 set = depth 2, etc.
    """

    rules = Rules()

    if concept == None:
        return rules

    # pdb.set_trace()

    concepts, edges, loadedConcepts = conceptnetreader.query_concept( concept, context=context, maxedges=maxedges, lang=lang, weight=0.5 )

    if not edges:
        return rules
    
    if not concepts:
        return rules

    context = concepts[0].get('context', '')

    for edge in edges:
        concept1lang,concept1name,relation,concept2lang,concept2name,weight,rev,sym = edge
        rule = Rule( concept1name, relation, concept2name, concept1lang, concept2lang, context, weight=weight )
        rules.append( rule )
        
    return rules

query = query_cnr

</t>
<t tx="karstenw.20230417110004.1"># for whoever effed up the def type() below...
typ = type

# code -&gt; id, name, autonym
languages = {}

# id -&gt; patternrelation
relations = {}

# contexts -&gt; id
contexts = {}


</t>
<t tx="karstenw.20230417110310.1"></t>
<t tx="karstenw.20230505102222.1"></t>
<t tx="karstenw.20230505121633.1">def cachefiles( old=True):
    
    if old:
        f = glob.glob( CACHE + '/*.*' )
    else:
        f = glob.glob( CACHE + '/??/*.*' )
    # a9fab0421559435f727de417975aa4d40f2547d7.json
    # pp(f)
    return f


</t>
</tnodes>
</leo_file>
